{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Setup nltk corpora path\n",
    "nltk_path = os.sep.join([os.environ['HOME'], 'nltk_data'])\n",
    "nltk.data.path.insert(0, nltk_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess_series_text(data, nltk_path=nltk_path):\n",
    "    \"\"\"Perform preprocessing on a Pandas series\n",
    "       including removal of alpha numerical words,\n",
    "       punctuation removal, tokenization, and stop word removal.\"\"\"\n",
    "    \n",
    "    # remove alpha numerical words and make lowercase\n",
    "    alphanum_re = re.compile(r\"\"\"\\w*\\d\\w*\"\"\")\n",
    "    data = alphanum_re.sub(\"\", data.strip().lower())\n",
    "\n",
    "    # remove punctuation\n",
    "    punc_re = re.compile('[%s]' % re.escape(string.punctuation))    \n",
    "    data = punc_re.sub(' ', data)\n",
    "\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    data = data.translate(translator)\n",
    "\n",
    "    # tokenize words    \n",
    "    words = word_tokenize(data)\n",
    "    \n",
    "    # remove stop words\n",
    "    sw = stopwords.words('english')\n",
    "    sw.extend([\"â€“\",'–',\"“\",\"“off\",\"“the\",\"“you’ve\",\"tv”\",'v',\"it’s\",\"i’ve\",\"“rss\",\"”\",'%',\"â€™\"])\n",
    "    sw.extend(['january','february','march','april','may','june','july','august','september','october','november','december'])   \n",
    "    sw.extend([\"that’s\",\"there’s'\",\"yesterday\",\"tomorrow\",\"today\",\"i’m\",\"“if\",\"here’s\"])\n",
    "    words = list(filter(lambda y: y not in sw, words))\n",
    "    data = ' '.join(words) # we want to returm data as text. i.e. string\n",
    "    \n",
    "    nouns = [token for token, pos in pos_tag(words) if pos.startswith('N')]\n",
    "    data_nouns = ' '.join(nouns)\n",
    "    \n",
    "    return data, data_nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining files from multiple courses into one: a file per year of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: techcrunch2006.csv  number of lines: 4562\n",
      "File: venturebeat2006.csv  number of lines: 1607\n",
      "File: techcrunch2007.csv  number of lines: 12463\n",
      "File: venturebeat2007.csv  number of lines: 3268\n",
      "File: techcrunch2008.csv  number of lines: 16576\n",
      "File: venturebeat2008.csv  number of lines: 5455\n",
      "File: techcrunch2009.csv  number of lines: 16234\n",
      "File: venturebeat2009.csv  number of lines: 8886\n",
      "File: techcrunch2010.csv  number of lines: 16139\n",
      "File: venturebeat2010.csv  number of lines: 9605\n",
      "File: techcrunch2011.csv  number of lines: 15265\n",
      "File: venturebeat2011.csv  number of lines: 9197\n",
      "File: techcrunch2012.csv  number of lines: 14432\n",
      "File: venturebeat2012.csv  number of lines: 13366\n",
      "File: techcrunch2013.csv  number of lines: 13764\n",
      "File: venturebeat2013.csv  number of lines: 13504\n",
      "File: techcrunch2014.csv  number of lines: 12928\n",
      "File: venturebeat2014.csv  number of lines: 13828\n",
      "File: techcrunch2015.csv  number of lines: 11974\n",
      "File: venturebeat2015.csv  number of lines: 13809\n",
      "File: techcrunch2016.csv  number of lines: 12673\n",
      "File: venturebeat2016.csv  number of lines: 11613\n",
      "File: techcrunch2017.csv  number of lines: 5596\n",
      "File: venturebeat2017.csv  number of lines: 5274\n",
      "Total number of articles: 262018\n"
     ]
    }
   ],
   "source": [
    "# The code below reads files scrapped from multiple sources, then combines data by year and saves it into a pkl file. \n",
    "\n",
    "years = ['2006','2007','2008','2009','2010','2011','2012','2013','2014','2015','2016','2017']\n",
    "\n",
    "# Count the total number of articles from all sources\n",
    "articles_total = 0\n",
    "\n",
    "for year in years:\n",
    "    \n",
    "    texts = []\n",
    "    dates = []\n",
    "    blobs = []\n",
    "    tags = []\n",
    "    text_nouns = []\n",
    "    df_tmp = pd.DataFrame()\n",
    "    \n",
    "    # The folder \"Data_AllSource\" contains 24 .csv files: 12 from TechCrunch website and 12 from Venturebeat website.\n",
    "    # They are the result of scraping both sites year by year.\n",
    "    # Their names contain the year, for example, \"techcrunch2006.csv\"\n",
    "    \n",
    "    for filename in os.listdir(os.getcwd()+\"/Data_AllSources/\"):\n",
    "        \n",
    "        if year in filename:\n",
    "            \n",
    "            with open(\"Data_AllSources/\"+filename,'r') as f:\n",
    "\n",
    "                df = pd.read_csv(f)\n",
    "                print(\"File:\",filename,\" number of lines:\",len(df))\n",
    "                articles_total = articles_total + len(df)\n",
    "\n",
    "                dates.append(df['date'])        \n",
    "                df['text'].fillna(\"\", inplace=True)\n",
    "                df['tags'].fillna(\"\", inplace=True)\n",
    "\n",
    "                for i in range(len(df)):\n",
    "\n",
    "                    text_tmp = df['text'][i]\n",
    "                    if isinstance(text_tmp, str):\n",
    "                        text_tmp = text_tmp.replace('\\n', ' ')\n",
    "                    else:\n",
    "                        text_tmp = str(text_tmp).replace('\\n', ' ')\n",
    "\n",
    "                    data, data_nouns = preprocess_series_text(text_tmp)\n",
    "                    texts.append(data)\n",
    "                    text_nouns.append(data_nouns)\n",
    "\n",
    "                    # to use in Approach 1    \n",
    "                    blob = TextBlob(data)\n",
    "                    blob_nouns = list(blob.noun_phrases)\n",
    "                    blobs.append(blob_nouns)\n",
    "\n",
    "                    # to use in Approach 2\n",
    "                    data_tags, data_tag_nouns = preprocess_series_text(df['tags'][i])\n",
    "                    blob = TextBlob(data_tags)\n",
    "                    blob_tags = list(blob.words)\n",
    "                    tags.append(blob_tags)\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    # writing texts and text_nouns from both files dated with the same year into one df        \n",
    "    df_tmp['text'] = texts\n",
    "    df_tmp['nouns'] = text_nouns\n",
    "    df_tmp.to_pickle(\"Data_AllSources/PKLs/\"+\"pkl\"+str(year)+\".pkl\")\n",
    "print(\"Total number of articles:\", articles_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listed below are attempts to retrieve specific information, i.e. technology names, from the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 1: Use noun phrases from TextBlob.\n",
    "#### Conclusion: the result looks noisy, the approach has been abandoned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# blobs_allDocs is a list of all noun phrases in all articles, each of which may occur multiple times\n",
    "blobs_allDocs = sum(blobs,[])\n",
    "# blobs_vocab is a list of all noun phrases in all articles, each phrase occurs only once\n",
    "blobs_vocab = list(set(blobs_allDocs))\n",
    "\n",
    "# Converting a collection of articles to a matrix of token counts using predefined vocabulary built from the noun phrases\n",
    "cv_noun_phrases = CountVectorizer(vocabulary=blobs_vocab)\n",
    "# Noun phrases feature matrix\n",
    "feature_matrix_noun_phrases = cv_noun_phrases.fit_transform(texts).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun phrases feature matrix size:  (6169, 114751)\n",
      "{'loss leaders —': 0, 'img src https files wordpress com mac pros lcds': 1, 'contact lenses': 2, 'heavy duty home': 3, '“that” peripheral value village year trust': 4, 'netscape update': 5, 'they’ll building': 6, 'toaster amps workspace roundup ergonomic chairs solar': 7, 'successful investments companies': 8, 'ati radeons nvidia geforces': 9, 'black lung we’re': 10, 'sides we’d': 11, 'transaction cilion denis segota matthew bartus': 12, 'site jigsaw isn’t': 13, 'webmethods inc': 14, 'personal decision': 15, 'real estate agents brokers': 16, 'open beta': 17, 'industries family tradition': 18, 'selection choice quotes': 19}\n"
     ]
    }
   ],
   "source": [
    "print(\"Noun phrases feature matrix size: \",feature_matrix_noun_phrases.shape)\n",
    "\n",
    "mydict_noun_phrases = cv_noun_phrases.vocabulary_\n",
    "print(dict(list(mydict_noun_phrases.items())[0:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 2: Use the words in the field \"tags\" from the scraped files or from the last sentence in the articles that starts with \"Tags:\" if present.\n",
    "#### Conclusion: tags are not consistent throughout the years, the approach has been abondoned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tags_allDocs is a list of tags from all articles, each of which may occur multiple times\n",
    "tags_allDocs = sum(tags,[])\n",
    "# tags_vocab is a list of all tags from all articles, each tag occurs only once\n",
    "tags_vocab = list(set(tags_allDocs))\n",
    "\n",
    "# Converting a collection of articles to a matrix of token counts using predefined vocabulary built from the tags\n",
    "cv_tags = CountVectorizer(vocabulary=tags_vocab)\n",
    "# Tags feature matrix\n",
    "feature_matrix_tags = cv_tags.fit_transform(texts).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags feature matrix size:  (6169, 1284)\n",
      "{'raid': 0, 'roundup': 1, 'webaroo': 2, 'keys': 3, 'sina': 4, 'headset': 5, 'york': 6, 'alcohol': 7, 'heart': 8, 'twitter': 9, 'kids': 10, 'semantic': 11, 'wink': 12, 'freecharge': 13, 'home': 14, 'myspace': 15, 'coffee': 16, 'estimates': 17, 'trusted': 18, 'zigtag': 19}\n"
     ]
    }
   ],
   "source": [
    "print(\"Tags feature matrix size: \",feature_matrix_tags.shape)\n",
    "\n",
    "mydict_tags = cv_tags.vocabulary_\n",
    "print(dict(list(mydict_tags.items())[0:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 3: Apply usual n-grams to noun-only texts.\n",
    "#### Conclusion: bigrmas (attempt 3.2) and trigrmas (attempt 3.3) are not what I need, but unigrams (attempt 3.1) show the promise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Attepmpt 3.1: apply usual n-grams to noun-only texts, unigrams\n",
    "cv_nouns_1 = CountVectorizer(ngram_range=(1,1)\n",
    "#                            ,min_df=0.01 # playing with parameters\n",
    "                           ,max_df=0.01\n",
    "                          )\n",
    "feature_matrix_nouns_1 = cv_nouns_1.fit_transform(text_nouns).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only-noun unigram feature matrix size:  (6169, 22145)\n",
      "{'collaborative': 3720, 'api': 788, 'airset': 406, 'write': 21740, 'traction': 20073, 'lucas': 11474, 'gonze': 8095, 'webjay': 21284, 'burton': 2598, 'playlists': 14716, 'honolulu': 9025, 'hawaii': 8643, 'developement': 5172, 'excuses': 6536, 'diligence': 5301, 'congratulation': 4037, 'riya': 16538, 'recovery': 15955, 'fiasco': 6941, 'killer': 10607}\n"
     ]
    }
   ],
   "source": [
    "print(\"Only-noun unigram feature matrix size: \",feature_matrix_nouns_1.shape)\n",
    "\n",
    "mydict_nouns_1 = cv_nouns_1.vocabulary_\n",
    "print(dict(list(mydict_nouns_1.items())[0:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Attempt 3.2: apply usual n-grams to noun-only texts, bigrams\n",
    "cv_nouns_2 = CountVectorizer(ngram_range=(2,2)\n",
    "                           ,min_df=0.01\n",
    "#                            ,max_df=0.001 # playing with parameters\n",
    "                          )\n",
    "feature_matrix_nouns_2 = cv_nouns_2.fit_transform(text_nouns).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only-noun bigram feature matrix size:  (6169, 46)\n",
      "{'cell phone': 3, 'they ve': 29, 'market share': 12, 'email address': 6, 'search engine': 22, 'search results': 23, 'home page': 7, 'venture capital': 31, 'page views': 16, 'business model': 0, 'silicon valley': 24, 'partners ventures': 18, 'they ll': 27, 'venture capitalists': 33, 'york times': 42, 'kleiner perkins': 11, 'mountain view': 15, 'we ve': 40, 'co founder': 4, 'we ll': 38}\n"
     ]
    }
   ],
   "source": [
    "print(\"Only-noun bigram feature matrix size: \",feature_matrix_nouns_2.shape)\n",
    "\n",
    "mydict_nouns_2 = cv_nouns_2.vocabulary_\n",
    "print(dict(list(mydict_nouns_2.items())[0:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Attempt 3.3: apply usual n-grams to noun-only texts, trigrams\n",
    "cv_nouns_3 = CountVectorizer(ngram_range=(3,3)\n",
    "#                            ,min_df=0.01\n",
    "#                            ,max_df=0.001\n",
    "                          )\n",
    "feature_matrix_nouns_3 = cv_nouns_3.fit_transform(text_nouns).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only-noun bigram feature matrix size:  (6169, 343293)\n",
      "{'collaborative calendar contacts': 46989, 'calendar contacts application': 31611, 'contacts application api': 60400, 'application api integration': 11906, 'api integration verizon': 11036, 'integration verizon yesterday': 137341, 'verizon yesterday access': 316918, 'yesterday access calendar': 339969, 'access calendar contact': 596, 'calendar contact information': 31610, 'contact information verizon': 60335, 'information verizon cell': 135928, 'verizon cell phone': 316824, 'cell phone services': 38641, 'phone services needs': 206898, 'services needs businesses': 255723, 'needs businesses office': 181623, 'businesses office standard': 30019, 'office standard services': 189572, 'standard services airset': 274537}\n"
     ]
    }
   ],
   "source": [
    "print(\"Only-noun bigram feature matrix size: \",feature_matrix_nouns_3.shape)\n",
    "\n",
    "mydict_nouns_3 = cv_nouns_3.vocabulary_\n",
    "print(dict(list(mydict_nouns_3.items())[0:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 4: Apply unsupervised clustering in hope to identify clusters and thus technologies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN did not produce any result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6169\n",
      "[-1 -1 -1 ..., -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "db = DBSCAN(eps=1.0, min_samples=10).fit(feature_matrix_nouns_1)\n",
    "labels = db.labels_\n",
    "print(len(labels))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KMeans did produce the result, but the result didn't make it to the final presentation. KMeans proved to be time-consuming, the algorithms is located in a separate notebook *TechNews_KMeans.ipynb*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
